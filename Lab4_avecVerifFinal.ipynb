{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Old1AA9lOqs-",
        "outputId": "2f7e8d58-32a4-4cae-d9bf-cd0deb853663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 24 02:07:54 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv3jslfEOvHr",
        "outputId": "365868bf-c465-41e0-dd31-a20a8b9ddaa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lab4_2.cu\n",
        "#include <cuda.h>\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "#include <stdio.h>\n",
        "#include <cassert>\n",
        "#include <iostream>\n",
        "using namespace std;\n",
        "\n",
        "#define BLOCK_SIZE 32 // Taille du bloc pour une bonne utilisation de la mémoire partagée\n",
        "\n",
        "// Kernel sans padding\n",
        "__global__ void matrixTransposeNoPadding(float* input, float* output, int width, int height) {\n",
        "    __shared__ float tile[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    int x = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n",
        "    int y = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n",
        "\n",
        "    if (x < width && y < height) {\n",
        "        tile[threadIdx.y][threadIdx.x] = input[y * width + x];\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    x = blockIdx.y * BLOCK_SIZE + threadIdx.x;\n",
        "    y = blockIdx.x * BLOCK_SIZE + threadIdx.y;\n",
        "\n",
        "    if (x < height && y < width) {\n",
        "        output[y * height + x] = tile[threadIdx.x][threadIdx.y];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel avec padding\n",
        "__global__ void matrixTransposeWithPadding(float* input, float* output, int width, int height) {\n",
        "    __shared__ float tile[BLOCK_SIZE][BLOCK_SIZE + 1]; // Ajout d'un padding pour éviter les conflits de banque\n",
        "\n",
        "    int x = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n",
        "    int y = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n",
        "\n",
        "    if (x < width && y < height) {\n",
        "        tile[threadIdx.y][threadIdx.x] = input[y * width + x];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    x = blockIdx.y * BLOCK_SIZE + threadIdx.x;\n",
        "    y = blockIdx.x * BLOCK_SIZE + threadIdx.y;\n",
        "\n",
        "    if (x < height && y < width) {\n",
        "        output[y * height + x] = tile[threadIdx.x][threadIdx.y];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel de réduction parallèle optimisée\n",
        "__global__ void reduceOptimized(const float* input, float* output, int n) {\n",
        "    __shared__ double sharedMem[BLOCK_SIZE]; // Mémoire partagée en double précision pour minimiser les erreurs\n",
        "\n",
        "    int tid = threadIdx.x;                // Identifiant du thread dans le bloc\n",
        "    int globalIdx = blockIdx.x * blockDim.x + threadIdx.x; // Index global\n",
        "\n",
        "    // Charger les données dans la mémoire partagée\n",
        "    sharedMem[tid] = (globalIdx < n) ? static_cast<double>(input[globalIdx]) : 0.0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Réduction dans la mémoire partagée\n",
        "    for (int stride = blockDim.x / 2; stride > 32; stride /= 2) {\n",
        "        if (tid < stride) {\n",
        "            sharedMem[tid] += sharedMem[tid + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Réduction finale dans un warp\n",
        "    if (tid < 32) {\n",
        "        double val = sharedMem[tid];\n",
        "        for (int offset = 16; offset > 0; offset /= 2) {\n",
        "            val += __shfl_down_sync(0xffffffff, val, offset);\n",
        "        }\n",
        "        sharedMem[tid] = val;\n",
        "    }\n",
        "\n",
        "    // Le thread 0 écrit le résultat final\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = static_cast<float>(sharedMem[0]);\n",
        "    }\n",
        "}\n",
        "\n",
        "void transposeVerifyCPU(float* input, float* output, int width, int height) {\n",
        "    for (int y = 0; y < height; y++) {\n",
        "        for (int x = 0; x < width; x++) {\n",
        "            assert(output[y * width + x] == input[x * height + y]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int width = 1024;\n",
        "    int height = 1024;\n",
        "\n",
        "    size_t size = width * height * sizeof(float);\n",
        "\n",
        "    float* h_input = (float*)malloc(size);\n",
        "    float* h_output = (float*)malloc(size);\n",
        "\n",
        "    // Remplir la matrice avec des valeurs arbitraires\n",
        "    for (int i = 0; i < width * height; i++) {\n",
        "        h_input[i] = static_cast<float>(i);\n",
        "    }\n",
        "\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, size);\n",
        "    cudaMalloc(&d_output, size);\n",
        "\n",
        "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 gridSize((width + BLOCK_SIZE - 1) / BLOCK_SIZE, (height + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    // Profiler et comparer les deux implémentations\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Transposition sans padding\n",
        "    cudaEventRecord(start);\n",
        "    matrixTransposeNoPadding<<<gridSize, blockSize>>>(d_input, d_output, width, height);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float noPaddingTime;\n",
        "    cudaEventElapsedTime(&noPaddingTime, start, stop);\n",
        "    printf(\"Temps sans padding : %f ms\\n\", noPaddingTime);\n",
        "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
        "    transposeVerifyCPU(h_input, h_output, width, height);\n",
        "\n",
        "    // Transposition avec padding\n",
        "    cudaEventRecord(start);\n",
        "    matrixTransposeWithPadding<<<gridSize, blockSize>>>(d_input, d_output, width, height);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float withPaddingTime;\n",
        "    cudaEventElapsedTime(&withPaddingTime, start, stop);\n",
        "    printf(\"Temps avec padding : %f ms\\n\", withPaddingTime);\n",
        "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
        "    transposeVerifyCPU(h_input, h_output, width, height);\n",
        "\n",
        "    // Réduction parallèle optimisée\n",
        "    int n = 1024;\n",
        "    size_t reductionSize = n * sizeof(float);\n",
        "    float* h_reductionInput = (float*)malloc(reductionSize);\n",
        "    float* h_reductionOutput = (float*)malloc(reductionSize / BLOCK_SIZE);\n",
        "\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_reductionInput[i] = 1.0f; // Exemple : Tableau rempli avec des 1\n",
        "    }\n",
        "\n",
        "    float *d_reductionInput, *d_reductionOutput;\n",
        "    cudaMalloc(&d_reductionInput, reductionSize);\n",
        "    cudaMalloc(&d_reductionOutput, reductionSize / BLOCK_SIZE);\n",
        "\n",
        "    cudaMemcpy(d_reductionInput, h_reductionInput, reductionSize, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 reductionBlockSize(BLOCK_SIZE);\n",
        "    dim3 reductionGridSize((n + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    reduceOptimized<<<reductionGridSize, reductionBlockSize>>>(d_reductionInput, d_reductionOutput, n);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float reductionTime;\n",
        "    cudaEventElapsedTime(&reductionTime, start, stop);\n",
        "    printf(\"Temps de réduction optimisée : %f ms\\n\", reductionTime);\n",
        "    cudaMemcpy(h_reductionOutput, d_reductionOutput, reductionSize / BLOCK_SIZE, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float reductionSum = 0;\n",
        "    for (int i = 0; i < reductionGridSize.x; i++) {\n",
        "        reductionSum += h_reductionOutput[i];\n",
        "    }\n",
        "    cout << \"ReductionSum of GPU : \" << reductionSum << endl;\n",
        "\n",
        "    float reductionSumCPU = 0;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        reductionSumCPU += h_reductionInput[i];\n",
        "    }\n",
        "    cout << \"Reduction of CPU : \" << reductionSumCPU << endl;\n",
        "\n",
        "    // Nettoyage\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_reductionInput);\n",
        "    cudaFree(d_reductionOutput);\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "    free(h_reductionInput);\n",
        "    free(h_reductionOutput);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq8vFeCaA-EW",
        "outputId": "e6142b39-ceb7-426f-be19-d786f43f87b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing lab4_2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -gencode=arch=compute_75,code=sm_75 lab4_2.cu -o lab4_2\n"
      ],
      "metadata": {
        "id": "FSzh0QN_BEgO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./lab4_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnNafjgXBEdZ",
        "outputId": "e87b8395-6f56-4b78-e924-4f605a6b263c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temps sans padding : 0.209632 ms\n",
            "Temps avec padding : 0.094400 ms\n",
            "Temps de réduction optimisée : 0.032704 ms\n",
            "ReductionSum of GPU : 1024\n",
            "Reduction of CPU : 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./lab4_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvwretuQBDFn",
        "outputId": "ed8b3ce1-c972-4eff-b1a4-c4b2d9424694"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==3764== NVPROF is profiling process 3764, command: ./lab4_2\n",
            "Temps sans padding : 0.341536 ms\n",
            "Temps avec padding : 0.115264 ms\n",
            "Temps de réduction optimisée : 0.053728 ms\n",
            "ReductionSum of GPU : 1024\n",
            "Reduction of CPU : 1024\n",
            "==3764== Profiling application: ./lab4_2\n",
            "==3764== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   48.98%  2.7054ms         3  901.81us  1.6640us  2.0390ms  [CUDA memcpy DtoH]\n",
            "                   47.38%  2.6170ms         2  1.3085ms  1.4720us  2.6156ms  [CUDA memcpy HtoD]\n",
            "                    2.44%  134.91us         1  134.91us  134.91us  134.91us  matrixTransposeNoPadding(float*, float*, int, int)\n",
            "                    1.13%  62.207us         1  62.207us  62.207us  62.207us  matrixTransposeWithPadding(float*, float*, int, int)\n",
            "                    0.08%  4.2560us         1  4.2560us  4.2560us  4.2560us  reduceOptimized(float const *, float*, int)\n",
            "      API calls:   95.77%  210.37ms         4  52.593ms  18.760us  210.00ms  cudaMalloc\n",
            "                    3.53%  7.7486ms         5  1.5497ms  24.711us  3.7897ms  cudaMemcpy\n",
            "                    0.30%  653.01us         4  163.25us  8.4220us  234.86us  cudaFree\n",
            "                    0.15%  331.99us         3  110.66us  46.148us  236.00us  cudaLaunchKernel\n",
            "                    0.10%  217.52us       114  1.9080us     185ns  84.327us  cuDeviceGetAttribute\n",
            "                    0.09%  200.56us         3  66.852us  6.8360us  133.29us  cudaEventSynchronize\n",
            "                    0.03%  60.221us         6  10.036us  4.1230us  18.628us  cudaEventRecord\n",
            "                    0.01%  28.417us         2  14.208us  1.6050us  26.812us  cudaEventCreate\n",
            "                    0.01%  13.659us         1  13.659us  13.659us  13.659us  cuDeviceGetName\n",
            "                    0.01%  13.401us         1  13.401us  13.401us  13.401us  cuDeviceGetPCIBusId\n",
            "                    0.00%  8.9620us         3  2.9870us  2.8700us  3.0810us  cudaEventElapsedTime\n",
            "                    0.00%  8.8850us         1  8.8850us  8.8850us  8.8850us  cuDeviceTotalMem\n",
            "                    0.00%  6.5060us         2  3.2530us  1.1960us  5.3100us  cudaEventDestroy\n",
            "                    0.00%  2.3570us         3     785ns     290ns  1.7340us  cuDeviceGetCount\n",
            "                    0.00%  1.3730us         2     686ns     366ns  1.0070us  cuDeviceGet\n",
            "                    0.00%     620ns         1     620ns     620ns     620ns  cuModuleGetLoadingMode\n",
            "                    0.00%     498ns         1     498ns     498ns     498ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ]
}